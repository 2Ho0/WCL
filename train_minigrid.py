import os
import shutil
import wandb  # Weights & Biases ë¡œê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬
import gym  # ê°•í™”í•™ìŠµ í™˜ê²½ ë¼ì´ë¸ŒëŸ¬ë¦¬
from gym_minigrid.wrappers import *  # MiniGrid í™˜ê²½ ê´€ë ¨ ë˜í¼
import dreamerv2.api as dv2  # DreamerV2 API
from input_args import parse_minigrid_args  # ì‹¤í–‰ ì‹œ ì…ë ¥ ì¸ì ì²˜ë¦¬

# âœ… TensorFlow GPU ì„¤ì • (ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹)
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu, True)

os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"  # TensorFlowì˜ GPU ë©”ëª¨ë¦¬ ì œí•œ í•´ì œ

# âœ… Weights & Biases ë¡œê·¸ì¸ (API í‚¤ í•„ìš”)
wandb.login(key="251cbda79884670a14d9e45aa41195e7e6730c92")

# ğŸ”¹ MiniGrid í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
def run_minigrid(args):
    tag = args.tag + "_" + str(args.seed)  # íƒœê·¸ + ëœë¤ ì‹œë“œ ê°’ ì„¤ì •

    # âœ… DreamerV2 ê¸°ë³¸ ì„¤ì •ê°’ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ í•™ìŠµ ì„¤ì • êµ¬ì„±
    config = dv2.defaults.update({
        'logdir': '{0}/minigrid_{1}'.format(args.logdir, tag),  # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        'log_every': 1e3,  # 1000 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡
        'log_every_video': 2e5,  # 200,000 ìŠ¤í…ë§ˆë‹¤ ë™ì˜ìƒ ì €ì¥
        'train_every': 10,  # 10 ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ìˆ˜í–‰
        'prefill': 1e4,  # í•™ìŠµ ì „ 10,000 ìŠ¤í… ë™ì•ˆ ë°ì´í„° ìˆ˜ì§‘
        'time_limit': 100,  # ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´
        'actor_ent': 3e-3,  # ì •ì±… ì—”íŠ¸ë¡œí”¼ ì¡°ì •
        'loss_scales.kl': 1.0,  # KL ì†ì‹¤ ìŠ¤ì¼€ì¼ë§
        'discount': 0.99,  # í• ì¸ìœ¨
        'steps': args.steps,  # ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜
        'cl': args.cl,  # Continual Learning ì‚¬ìš© ì—¬ë¶€
        'num_tasks': args.num_tasks,  # í•™ìŠµí•  íƒœìŠ¤í¬ ê°œìˆ˜
        'num_task_repeats': args.num_task_repeats,  # íƒœìŠ¤í¬ ë°˜ë³µ íšŸìˆ˜
        'seed': args.seed,  # ëœë¤ ì‹œë“œ
        'eval_every': 1e4,  # 10,000 ìŠ¤í…ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰
        'eval_steps': 1e3,  # í‰ê°€ ì‹œ 1,000 ìŠ¤í… ì‚¬ìš©
        'tag': tag,  # íƒœê·¸ ì„¤ì •
        "unbalanced_steps": args.unbalanced_steps,
        'replay.capacity': args.replay_capacity,  # ë¦¬í”Œë ˆì´ ë²„í¼ ìš©ëŸ‰
        'replay.reservoir_sampling': args.reservoir_sampling,  # ë¦¬ì €ë²„ ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€
        'replay.recent_past_sampl_thres': args.recent_past_sampl_thres,  # ìµœê·¼ ìƒ˜í”Œ ì„ê³„ê°’
        'sep_exp_eval_policies': args.sep_exp_eval_policies,  # í‰ê°€ ì‹œ ê°œë³„ ì •ì±… ì‚¬ìš© ì—¬ë¶€
        'replay.minlen': args.minlen,  # ë¦¬í”Œë ˆì´ ë²„í¼ ìµœì†Œ ê¸¸ì´
        'wandb.group': args.wandb_group,  # W&B ê·¸ë£¹
        'wandb.name': "mwilliam55",  # W&B ì‹¤í—˜ ì´ë¦„
        'wandb.project': "wcl",  # W&B í”„ë¡œì íŠ¸ ì´ë¦„
        'wandb.entity': 'hyeonglee-dku',  # W&B ì—”í„°í‹° ì„¤ì •
    }).parse_flags()

    # ğŸ”¹ Plan2Explore (íƒìƒ‰ ì¤‘ì‹¬ í•™ìŠµ) í™œì„±í™” ì‹œ ì¶”ê°€ ì„¤ì •
    if args.plan2explore:
        config = config.update({
            'wandb.entity': 'hyeonglee-dku',
            'expl_behavior': 'Plan2Explore',
            'pred_discount': args.rssm_full_recon,
            'grad_heads': ['decoder', 'reward', 'discount'] if args.rssm_full_recon else ['decoder'],
            'expl_intr_scale': args.expl_intr_scale,
            'expl_extr_scale': args.expl_extr_scale,
            'expl_every': args.expl_every,
            'discount': 0.99,
            'wandb.name': "mwilliam55"
        }).parse_flags()

    # âœ… W&B ì‹¤í—˜ ì‹œì‘
    wandb.init(
        config=config,
        reinit=True,
        resume=False,
        sync_tensorboard=True,
        dir=args.wandb_dir,
        **config.wandb,
    )

    # ğŸ”¹ Continual Learning ëª¨ë“œ (CL í•™ìŠµ ì‹¤í–‰)
    if config.cl:
        env_names = [
            'MiniGrid-DoorKey-8x8-v0',
            'MiniGrid-LavaCrossingS9N1-v0',
            'MiniGrid-SimpleCrossingS9N1-v0',
        ]

        envs = []
        for i in range(config.num_tasks):
            name = env_names[i]
            env = gym.make(name)
            env = RGBImgPartialObsWrapper(env)  # 'mission' í•„ë“œ ì œê±°
            if args.state_bonus:
                assert not args.plan2explore, "state bonusì™€ plan2exploreëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"
                env = StateBonus(env)
            envs.append(env)

        # ğŸ”¹ í‰ê°€ í™˜ê²½ ì„¤ì • (MultiSkill Task í¬í•¨)
        if args.eval_skills:
            env_names.append('MiniGrid-MultiSkill-N2-v0')  # ë‹¤ì¤‘ ê¸°ìˆ  í‰ê°€ í™˜ê²½ ì¶”ê°€

        eval_envs = []
        for name in env_names:
            env = gym.make(name)
            env = RGBImgPartialObsWrapper(env)
            eval_envs.append(env)

        dv2.cl_train_loop(envs, config, eval_envs=eval_envs)

    # ğŸ”¹ ë‹¨ì¼ í™˜ê²½ í•™ìŠµ ëª¨ë“œ
    else:
        env_names = [
            'MiniGrid-DoorKey-8x8-v0',
            'MiniGrid-LavaCrossingS9N1-v0',
            'MiniGrid-SimpleCrossingS9N1-v0',
        ]

        name = env_names[args.env]
        env = gym.make(name)
        env = RGBImgPartialObsWrapper(env)
        dv2.train(env, config)

    # âœ… í•™ìŠµ ë°ì´í„° ì‚­ì œ (ì˜µì…˜)
    if args.del_exp_replay:
        shutil.rmtree(os.path.join(config['logdir'], 'train_episodes'))

    # âœ… W&B ë¡œê¹… ì¢…ë£Œ
    wandb.finish()

# ğŸ”¹ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    args = parse_minigrid_args()  # ì‹¤í–‰ ì‹œ ì…ë ¥ëœ ì¸ì íŒŒì‹±
    run_minigrid(args)  # MiniGrid í•™ìŠµ ì‹¤í–‰
