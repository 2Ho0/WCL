action_repeat: 1
actor: {act: elu, dist: auto, layers: 4, min_std: 0.1, norm: layer, units: 400}
actor_ent: 0.002
actor_grad: auto
actor_grad_mix: 0.1
actor_opt: {clip: 100, eps: 1e-05, lr: 0.0001, opt: adam, wd: 1e-06}
atari_grayscale: true
cl: true
cl_small: true
clip_rewards: tanh
critic: {act: elu, dist: mse, layers: 4, norm: layer, units: 400}
critic_opt: {clip: 100, eps: 1e-05, lr: 0.0001, opt: adam, wd: 1e-06}
dataset: {batch: 16, length: 50}
decoder:
  act: elu
  cnn_depth: 48
  cnn_kernels: [5, 5, 6, 6]
  cnn_keys: image
  mlp_keys: $^
  mlp_layers: [400, 400, 400, 400]
  norm: layer
disag_action_cond: true
disag_log: false
disag_models: 10
disag_offset: 1
disag_target: stoch
discount: 0.99
discount_head: {act: elu, dist: binary, layers: 4, norm: layer, units: 400}
discount_lambda: 0.95
dmc_camera: -1
encoder:
  act: elu
  cnn_depth: 48
  cnn_kernels: [4, 4, 4, 4]
  cnn_keys: image
  mlp_keys: $^
  mlp_layers: [400, 400, 400, 400]
  norm: layer
env_seeds: [100]
envs: 1
envs_parallel: none
eval_eps: 1
eval_every: 50000.0
eval_noise: 0.0
eval_state_mean: false
eval_steps: 1000.0
expl_behavior: greedy
expl_every: 0
expl_extr_scale: 0.0
expl_head: {act: elu, dist: mse, layers: 4, norm: layer, units: 400}
expl_intr_scale: 1.0
expl_model_loss: kl
expl_noise: 0.0
expl_opt: {clip: 100, eps: 1e-05, lr: 0.0003, opt: adam, wd: 1e-06}
expl_reward_norm: {eps: 1e-08, momentum: 1.0, scale: 1.0}
expl_until: 0
grad_heads: [decoder, reward, discount]
imag_horizon: 15
jit: true
kl: {balance: 0.8, forward: false, free: 0.0, free_avg: true}
log_every: 1000.0
log_every_video: 200000.0
log_keys_max: ^log_achievement_.*
log_keys_mean: ^$
log_keys_sum: ^log_reward$
log_keys_video: [image]
log_recon_every: 100000.0
logdir: logs_cl/minihack_mh_cl_small_cm_s0_1M0
loss_scales: {discount: 1.0, kl: 1.0, proprio: 1.0, reward: 1.0}
model_opt: {clip: 100, eps: 1e-05, lr: 0.0001, opt: adam, wd: 1e-06}
num_task_repeats: 1
num_tasks: 4
precision: 16
pred_discount: true
prefill: 10000
pretrain: 1
render_size: [64, 64]
replay:
  capacity: 1000000.0
  coverage_sampling: true
  coverage_sampling_args:
    distance: euclid
    filters: 64
    kernel_size: [3, 3]
    normalize_lstm_state: true
    number_of_comparisons: 1000
  maxlen: 50
  minlen: 5
  ongoing: false
  prioritize_ends: true
  recent_past_sampl_thres: 0.0
  reservoir_sampling: false
  reward_sampling: false
  uncertainty_recalculation: 5000
  uncertainty_sampling: false
reward_head: {act: elu, dist: mse, layers: 4, norm: layer, units: 400}
reward_norm: {eps: 1e-08, momentum: 1.0, scale: 1.0}
rssm: {act: elu, deter: 1024, discrete: 32, ensemble: 1, hidden: 1024, min_std: 0.1,
  norm: layer, std_act: sigmoid2, stoch: 32}
seed: 0
sep_exp_eval_policies: true
skipped_metrics: [reward_normed_std, model_grad_norm, actor_grad_norm, critic_grad_norm,
  reward_std, reward_mean, reward_normed_mean]
slow_baseline: true
slow_target: true
slow_target_fraction: 1
slow_target_update: 100
steps: 1000000.0
tag: mh_cl_small_cm_s0_1M0
task: crafter_reward
time_limit: 100
train_every: 10
train_steps: 1
unbalanced_steps: None
wandb: {entity: hyeonglee-dku, group: experiment, mode: online, name: greedy_cl-small=True_mh_cl_small_cm_s0_1M0,
  notes: null, project: minihack_task_dist, tags: null}
